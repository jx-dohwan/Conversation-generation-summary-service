# -*- coding: utf-8 -*-
"""domain_adaptation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TbKA9pF7P4C8iJqWgCsqqEPoSlrVCfMH
"""

!pip install transformers==4.25.1
!pip install datasets==2.8.0
!pip install rouge
!pip install wandb

import random
import datasets
import transformers
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import numpy as np
import itertools
import re
import os
import json
from rouge import Rouge
from transformers import (
    AutoModelForSeq2SeqLM,
    AutoTokenizer,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
    DataCollatorForSeq2Seq,
    TrainingArguments,
    DataCollatorForLanguageModeling,
    LineByLineTextDataset,
    EarlyStoppingCallback,
    BartConfig,
    BartForConditionalGeneration,
    AutoModelForMaskedLM


)
from datasets import Dataset
from tqdm import tqdm
import torch

def data_mining(data):
  body_list = []
  text_list = []
  total_list = []
  with open(data) as f:
        data = json.load(f)
  for datum in data["data"]:
      body_list.append(datum['body'])  
  for text in body_list:
    text_list.append(text)
  for utt in text_list:
      utt_list = []
      for i in range(len(utt)):
          utt_list.append(utt[i]['utterance'])
      total_list.append(utt_list)
  return total_list

def data_load(filename):
    text = []

    for file in tqdm(filename):
      total_list = data_mining(file)
      for data in total_list:
        text.append("[sep]".join(data))

    return text

# def data_mining2(data):
#   body_list = []
#   text_list = []
#   total_list = []
#   with open(data) as f:
#         data = json.load(f)
#   for datum in data["data"]:
#       body_list.append(datum['body'])  
#   for text in body_list:
#     text_list.append(text)
#   for utt in text_list:
#       utt_list = []
#       for i in range(len(utt)):
#           utt_list.append(utt[i]['utterance'])
#       total_list.append(utt_list)
#   return total_list[:582]

# def data_load2(filename):
#     text = []

#     for file in tqdm(filename):
#       total_list = data_mining2(file)
#       for data in total_list:
#         text.append("[sep]".join(data))

#     return text

dirname = "/content/drive/MyDrive/인공지능/생성요약프로젝트/data/[라벨]한국어SNS_train"
filenames = os.listdir(dirname) 
train_full_filename = []

for filename in filenames:
    fn = os.path.join(dirname, filename)
    if dirname + '/.ipynb_checkpoints' != fn:
        train_full_filename.append(fn)

dirname2 = "/content/drive/MyDrive/인공지능/생성요약프로젝트/data/[라벨]한국어SNS_valid"
filenames2 = os.listdir(dirname2) 
val_full_filename = []

for filename in filenames2:
    fn2 = os.path.join(dirname2, filename)
    if dirname + '/.ipynb_checkpoints' != fn2:
        val_full_filename.append(fn2)

def preprocess_sentence(sentence):
    sentence = sentence.lower() # 텍스트 소문자화
    sentence = re.sub(r'[ㄱ-ㅎㅏ-ㅣ]+[/ㄱ-ㅎㅏ-ㅣ]', '', sentence) # 여러개 자음과 모음을 삭제한다.
    sentence = re.sub("[^가-힣a-z0-9#@,-\[\]\(\)]", " ", sentence) # 영어 외 문자(숫자, 특수문자 등) 공백으로 변환
    sentence = re.sub(r'[" "]+', " ", sentence) # 여러개 공백을 하나의 공백으로 바꿉니다.
    sentence = sentence.strip() # 문장 양쪽 공백 제거
    
    return sentence

def data_process(data):
  # 전체 Text 데이터에 대한 전처리 (1)
  text = []

  for data_text in tqdm(data):
    text.append(preprocess_sentence(data_text))
  
  return text

train_dataset = data_load(train_full_filename)
train_list = data_process(train_dataset)

val_dataset = data_load(val_full_filename)
val_list = data_process(val_dataset)

train_df = pd.DataFrame(zip(train_list), columns=['Text'])
val_df = pd.DataFrame(zip(val_list), columns=['Text'])

train_df.head()

# DF > data Set으로 전환
train_data = Dataset.from_pandas(train_df) 
val_data = Dataset.from_pandas(val_df)

print(train_data)
print(val_data)



model_checkpoints = "/content/drive/MyDrive/인공지능/생성요약프로젝트/Model/KoBART/checkpoint/domain_adaptation/checkpoint-12500"


tokenizer = AutoTokenizer.from_pretrained(model_checkpoints)
model = AutoModelForMaskedLM.from_pretrained(model_checkpoints)

special_words = [
                "#@주소#", "#@이모티콘#", "#@이름#", "#@URL#", "#@소속#",
                "#@기타#", "#@전번#", "#@계정#", "#@url#", "#@번호#", "#@금융#", "#@신원#",
                "#@장소#", "#@시스템#사진#", "#@시스템#동영상#", "#@시스템#기타#", "#@시스템#검색#",
                "#@시스템#지도#", "#@시스템#삭제#", "#@시스템#파일#", "#@시스템#송금#", "#@시스템#",
                "#개인 및 관계#", "#미용과 건강#", "#상거래(쇼핑)#", "#시사/교육#", "#식음료#", 
                "#여가 생활#", "#일과 직업#", "#주거와 생활#", "#행사#","[sep]"
                ]

tokenizer.add_special_tokens({"additional_special_tokens": special_words})
model.resize_token_embeddings(len(tokenizer))

def get_quartiles(lst):
  q1 = np.percentile(lst, 25)
  q2 = np.percentile(lst, 50)
  q3 = np.percentile(lst, 75)
  return q1, q2, q3

#t_len = [len(tokenizer.encode(s)) for s in tqdm(train_df['Text'])]
#q1, q2, q3 = get_quartiles(t_len)
#print("Q1:", q1)
#print("Q2:", q2)
#print("Q3:", q3)
#print("mean",sum(t_len) / len(t_len))

max_input = 128
max_target = 128
ignore_index = -100# tokenizer.pad_token_id
masking_rate: float = 0.15

def add_ignored_data(inputs, max_len, ignore_index):
  if len(inputs) < max_len:
      pad = [ignore_index] *(max_len - len(inputs)) # ignore_index즉 -100으로 패딩을 만들 것인데 max_len - lne(inpu)
      inputs = np.concatenate([inputs, pad])
  else:
      inputs = inputs[:max_len]

  return inputs

def add_padding_data(inputs, max_len, is_masking=False):
    pad_index = tokenizer.pad_token_id
    if is_masking:
        masked_index = torch.rand(len(inputs)) < masking_rate
        for i, m in enumerate(masked_index):
          if m:
            inputs[i] = tokenizer.mask_token_id
    if len(inputs) < max_len:
        pad = [pad_index] * (max_len - len(inputs))
        inputs = np.concatenate([inputs, pad])
    else:
        inputs = inputs[:max_len]

    return inputs 


def preprocess_data(data_to_process):
    label_id= []
    label_ids = []
    dec_input_ids = []
    input_ids = []
    bos = tokenizer('<s>')['input_ids']
    for i in range(len(data_to_process['Text'])):
        input_ids.append(add_padding_data(tokenizer.encode(data_to_process['Text'][i], add_special_tokens=False), max_input, is_masking=True))
    for i in range(len(data_to_process['Text'])):
        label_id.append(tokenizer.encode(data_to_process['Text'][i]))  
        label_id[i].append(tokenizer.eos_token_id)   
        dec_input_id = bos
        dec_input_id += label_id[i][:-1]
        dec_input_ids.append(add_padding_data(dec_input_id, max_target))  
    for i in range(len(data_to_process['Text'])):
        label_ids.append(add_ignored_data(label_id[i], max_target, ignore_index))
   
    return {'input_ids': input_ids,
            'attention_mask' : (np.array(input_ids) != tokenizer.pad_token_id).astype(int),
            'decoder_input_ids': dec_input_ids,
            'decoder_attention_mask': (np.array(dec_input_ids) != tokenizer.pad_token_id).astype(int),
            'labels': label_ids}

train_tokenize_data = train_data.map(preprocess_data, batched = True, remove_columns=['Text'])
val_tokenize_data = val_data.map(preprocess_data, batched = True, remove_columns=['Text'])

rouge = Rouge()
def compute_metrics(pred):
    labels_ids = pred.label_ids
    pred_ids = pred.predictions
    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
    labels_ids[labels_ids == -100] = tokenizer.pad_token_id
    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)
    
    return rouge.get_scores(pred_str, label_str, avg=True)

model.config.max_length = 128 
model.config.early_stopping = True
model.config.no_repeat_ngram_size = 3
model.config.length_penalty = 2.0
model.config.num_beams = 5

training_args = Seq2SeqTrainingArguments(
    output_dir="/content/drive/MyDrive/인공지능/생성요약프로젝트/Model/KoBART/checkpoint/domain_adaptation2",
    num_train_epochs=3,  # demo
    do_train=True,
    do_eval=True,
    per_device_train_batch_size=128,  
    per_device_eval_batch_size=256,
    learning_rate=3e-05,
    weight_decay=0.1,
    #label_smoothing_factor=0.1,
    predict_with_generate=True, # 생성기능을 사용하고 싶다고 지정한다.
    logging_dir="/content/drive/MyDrive/인공지능/생성요약프로젝트/Model/KoBART/logs2",
    save_total_limit=3,
    load_best_model_at_end = True,
    logging_strategy = 'epoch',
    evaluation_strategy  = 'epoch',
    save_strategy ='epoch',
)

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model) # 데이터 일괄 처리?

trainer = Seq2SeqTrainer(
    model, 
    training_args,
    train_dataset=train_tokenize_data,
    eval_dataset=val_tokenize_data,
    data_collator=data_collator,
    tokenizer=tokenizer,
    #compute_metrics=compute_metrics,
    callbacks = [EarlyStoppingCallback(early_stopping_patience=2)]
)

trainer.train()

"""
|epoch|	Training Loss|	Validation Loss|비고|
|:-----:|:--------------|:-----------------:|:----:|
|1	|0.410600|	0.368527|new masking|
|2|	0.369800|	0.345249|new masking|
|3|	0.370500|	0.344791|new masking|
|4|	0.341400|	0.334510||
|5|	0.326700|	0.331053||"""

train_tokenize_data.to_csv("/content/drive/MyDrive/인공지능/생성요약프로젝트/data/train_tokenize_data.csv", index=False)
val_tokenize_data.to_csv("/content/drive/MyDrive/인공지능/생성요약프로젝트/data/val_tokenize_data_data.csv", index=False)

model_checkpoints = "/content/drive/MyDrive/인공지능/생성요약프로젝트/Model/KoBART/checkpoint/domain_adaptation/checkpoint-12500"


tokenizer = AutoTokenizer.from_pretrained(model_checkpoints)
model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoints)



