{
  "nbformat": 5.8.0,
  "nbformat_minor": 4,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# KoBART_Summary_v3"
      ],
      "metadata": {
        "id": "_4RpbmcC0NSw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.25.1\n",
        "!pip install datasets==2.8.0\n",
        "!pip install rouge\n",
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "739b0d77-7cc6-41bf-8a20-127d03fdbc2f",
        "id": "gGBz6yCYh8vQ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.25.1\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers==4.25.1) (21.3)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m111.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==4.25.1) (4.64.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.25.1) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.25.1) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.25.1) (3.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==4.25.1) (2.25.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.25.1) (2022.6.2)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.25.1) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers==4.25.1) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.25.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.25.1) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.25.1) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.25.1) (1.24.3)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.12.0 tokenizers-0.13.2 transformers-4.25.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets==2.8.0\n",
            "  Downloading datasets-2.8.0-py3-none-any.whl (452 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.9/452.9 KB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 KB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 KB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets==2.8.0) (4.64.1)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets==2.8.0) (2022.11.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets==2.8.0) (2.25.1)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets==2.8.0) (9.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets==2.8.0) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets==2.8.0) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from datasets==2.8.0) (0.12.0)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets==2.8.0) (0.3.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets==2.8.0) (1.21.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets==2.8.0) (3.8.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets==2.8.0) (1.3.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.8.0) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.8.0) (1.3.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.8.0) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.8.0) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.8.0) (1.8.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.8.0) (2.1.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.8.0) (22.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.8.0) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.8.0) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->datasets==2.8.0) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets==2.8.0) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets==2.8.0) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets==2.8.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets==2.8.0) (4.0.0)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets==2.8.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets==2.8.0) (2022.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets==2.8.0) (1.15.0)\n",
            "Installing collected packages: xxhash, urllib3, multiprocess, responses, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed datasets-2.8.0 multiprocess-0.70.14 responses-0.18.0 urllib3-1.26.14 xxhash-3.2.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from rouge) (1.15.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.13.9-py2.py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from wandb) (4.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.19.6)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from wandb) (6.0)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.30-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.0/184.0 KB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.8/dist-packages (from wandb) (1.4.4)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (2.25.1)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.14.0-py2.py3-none-any.whl (178 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.9/178.9 KB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.15.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.14)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=dca6306d6ff021cdb6567ed235f195913c9eeb2f66fc243bf9239936f43d973f\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/8e/7e/72fbc243e1aeecae64a96875432e70d4e92f3d2d18123be004\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.30 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.14.0 setproctitle-1.3.2 smmap-5.0.0 wandb-0.13.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oP7ye9Nmh8vR"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "import transformers\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import itertools\n",
        "import re\n",
        "import torch\n",
        "from rouge import Rouge\n",
        "from transformers import (\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoTokenizer,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    EarlyStoppingCallback\n",
        "\n",
        ")\n",
        "from datasets import Dataset\n",
        "from tqdm import tqdm\n",
        "import glob\n",
        "import json\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_json_data(path):\n",
        "\n",
        "    with open(path) as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    ids = []\n",
        "    dialogues = []\n",
        "    summaries = []\n",
        "    topic = []\n",
        "    for datum in data[\"data\"]:\n",
        "        ids.append(datum[\"header\"][\"dialogueInfo\"][\"dialogueID\"])\n",
        "\n",
        "        prev_speaker_id = None\n",
        "        prev_line = \"\"\n",
        "        utts = []\n",
        "        for dialogue in datum[\"body\"][\"dialogue\"]:\n",
        "            utterance = dialogue[\"utterance\"].strip()\n",
        "\n",
        "            if dialogue[\"participantID\"] == prev_speaker_id:\n",
        "                prev_line += \" \" + utterance\n",
        "            else:\n",
        "                if prev_line:\n",
        "                    utts.append(prev_line)\n",
        "                prev_line = utterance\n",
        "                prev_speaker_id = dialogue[\"participantID\"]\n",
        "        if prev_line:\n",
        "            utts.append(prev_line)\n",
        "\n",
        "        dialogues.append(utts)\n",
        "        summaries.append(datum[\"body\"].get(\"summary\"))\n",
        "\n",
        "    for i in range(len(data['data'])):\n",
        "      topic.append(data['data'][i]['header']['dialogueInfo']['topic'])\n",
        "    return ids, dialogues, summaries, topic\n",
        "\n",
        "def data_load(filename, is_meta=False):\n",
        "    ids_list, dialogues_list, summaries_list, topic_list = [], [], [], []\n",
        "    dialogues_sep = []\n",
        "\n",
        "    for file in tqdm(filename):\n",
        "      ids, dialogues, summaries, topic = load_json_data(file)\n",
        "      for id, text, summ, top in zip(ids, dialogues, summaries, topic):\n",
        "        ids_list.append(id)\n",
        "        if is_meta:\n",
        "          text.insert(0,\"#\"+top+\"#\")\n",
        "        dialogues_list.append(text)\n",
        "        summaries_list.append(summ)\n",
        "        topic_list.append(top)\n",
        "    \n",
        "    for text in tqdm(dialogues_list):\n",
        "      dialogues_sep.append(\"[sep]\".join(text))\n",
        "\n",
        "    return ids_list, dialogues_sep, summaries_list\n"
      ],
      "metadata": {
        "id": "OYA0f5A7PMI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ay6kW9O2h8vV"
      },
      "outputs": [],
      "source": [
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower() # 텍스트 소문자화\n",
        "    sentence = re.sub(r'[ㄱ-ㅎㅏ-ㅣ]+[/ㄱ-ㅎㅏ-ㅣ]', '', sentence) # 여러개 자음과 모음을 삭제한다.\n",
        "    sentence = re.sub(\"[^가-힣a-z0-9#@,-\\[\\]\\(\\)]\", \" \", sentence) # 영어 외 문자(숫자, 특수문자 등) 공백으로 변환\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 여러개 공백을 하나의 공백으로 바꿉니다.\n",
        "    sentence = sentence.strip() # 문장 양쪽 공백 제거\n",
        "    \n",
        "    return sentence\n",
        "\n",
        "def data_process(data):\n",
        "  # 전체 Text 데이터에 대한 전처리 (1)\n",
        "  text = []\n",
        "\n",
        "  for data_text in tqdm(data):\n",
        "    text.append(preprocess_sentence(data_text))\n",
        "  \n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dirname = \"/content/drive/MyDrive/인공지능/아이펠톤/PoC/kt_data/Training\"\n",
        "filenames = os.listdir(dirname) \n",
        "train_full_filename = []\n",
        "\n",
        "for filename in filenames:\n",
        "    fn = os.path.join(dirname, filename)\n",
        "    if dirname + '/.ipynb_checkpoints' != fn:\n",
        "        train_full_filename.append(fn)\n",
        "\n",
        "dirname2 = \"/content/drive/MyDrive/인공지능/아이펠톤/PoC/kt_data/Validation\"\n",
        "filenames2 = os.listdir(dirname2) \n",
        "val_full_filename = []\n",
        "\n",
        "for filename in filenames2:\n",
        "    fn2 = os.path.join(dirname2, filename)\n",
        "    if dirname + '/.ipynb_checkpoints' != fn2:\n",
        "        val_full_filename.append(fn2)"
      ],
      "metadata": {
        "id": "gtZZo7ZMPCGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ids, train_dialogues, train_summaries = data_load(train_full_filename, is_meta=True)\n",
        "val_ids, val_dialogues, val_summaries = data_load(val_full_filename)\n",
        "\n",
        "train_texts = data_process(train_dialogues)\n",
        "val_texts = data_process(val_dialogues)\n",
        "\n",
        "train_df = pd.DataFrame(zip(train_texts,train_summaries), columns=['Text', 'Summary'])\n",
        "val_df = pd.DataFrame(zip(val_texts,val_summaries), columns=['Text', 'Summary'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2jsVHr1PCDj",
        "outputId": "3b81cfa1-4f9b-4f55-eb44-9a81e6a012cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9/9 [00:32<00:00,  3.58s/it]\n",
            "100%|██████████| 279992/279992 [00:00<00:00, 961209.83it/s]\n",
            "100%|██████████| 9/9 [00:07<00:00,  1.27it/s]\n",
            "100%|██████████| 35004/35004 [00:00<00:00, 948579.99it/s]\n",
            "100%|██████████| 279992/279992 [00:06<00:00, 44180.76it/s]\n",
            "100%|██████████| 35004/35004 [00:00<00:00, 47116.03it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "sjkwvt6Hf0ge",
        "outputId": "dc02a764-0f5a-4d5f-e805-34bbed5dd54e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                Text  \\\n",
              "0  #상거래(쇼핑)#[sep]그럼 날짜는 가격 큰 변동 없으면 6.28-7.13로 확정...   \n",
              "1  #상거래(쇼핑)#[sep]kf마스크만 5부제 하는거지?[sep]응. 면마스크는 아무...   \n",
              "2  #상거래(쇼핑)#[sep]아 근데 케이크 업체들 봤는데 중앙동쪽 거기는 맛만있고 디...   \n",
              "3  #상거래(쇼핑)#[sep]칫솔사야하는데 쓱으로 살까?[sep]뭘 칫솔사는것까지 물어...   \n",
              "4  #상거래(쇼핑)#[sep]잠도안오네 얼릉 고구마츄 먹고싶단[sep]그게 그렇게 맛있...   \n",
              "\n",
              "                                             Summary  \n",
              "0               비행기 표 가격에 대해 이야기하며, 특가 이벤트를 기다리고 있다.  \n",
              "1                비염이 있어서 싸게 나온 일회용 부직포 마스크를 사두려고 한다.  \n",
              "2  케이크 업체 중 중앙동 쪽은 맛만 있고 디자인은 별로고 고잔동 케이크 업체는 배달도...  \n",
              "3            칫솔을 3개월에 하나씩 바꿔서 왕 칫솔 사러 신세계(쓱) 가자고 했다.  \n",
              "4                  잠도 안 와서 고구마 말랭이를 양심상 하나만 먹으려고 한다.  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-202479c0-f5d0-4250-8eed-744e1a7ecd0a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>#상거래(쇼핑)#[sep]그럼 날짜는 가격 큰 변동 없으면 6.28-7.13로 확정...</td>\n",
              "      <td>비행기 표 가격에 대해 이야기하며, 특가 이벤트를 기다리고 있다.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>#상거래(쇼핑)#[sep]kf마스크만 5부제 하는거지?[sep]응. 면마스크는 아무...</td>\n",
              "      <td>비염이 있어서 싸게 나온 일회용 부직포 마스크를 사두려고 한다.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>#상거래(쇼핑)#[sep]아 근데 케이크 업체들 봤는데 중앙동쪽 거기는 맛만있고 디...</td>\n",
              "      <td>케이크 업체 중 중앙동 쪽은 맛만 있고 디자인은 별로고 고잔동 케이크 업체는 배달도...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>#상거래(쇼핑)#[sep]칫솔사야하는데 쓱으로 살까?[sep]뭘 칫솔사는것까지 물어...</td>\n",
              "      <td>칫솔을 3개월에 하나씩 바꿔서 왕 칫솔 사러 신세계(쓱) 가자고 했다.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>#상거래(쇼핑)#[sep]잠도안오네 얼릉 고구마츄 먹고싶단[sep]그게 그렇게 맛있...</td>\n",
              "      <td>잠도 안 와서 고구마 말랭이를 양심상 하나만 먹으려고 한다.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-202479c0-f5d0-4250-8eed-744e1a7ecd0a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-202479c0-f5d0-4250-8eed-744e1a7ecd0a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-202479c0-f5d0-4250-8eed-744e1a7ecd0a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "BjUdwNem_YFy",
        "outputId": "501c27dc-69dd-4888-c726-e060e756c54b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                Text  \\\n",
              "0  웅[sep]영업팀과장님이 보내줬는데 팀장님이 해줄지 모르겠다 저번에 부산갈때도 숙소...   \n",
              "1  너는 잘가라....회사.... 선택 잘해..[sep]알겠어 많이 힘들구나... 나도...   \n",
              "2  느낌상 대통령까지는 아니고 오시면 여사님정도오시지않을까 [sep]그러면서[sep]샘...   \n",
              "3  숨만수이 도 숨만쉬어도 100 이내[sep]한달안에 일 무조건 해야대[sep]아 딱...   \n",
              "4  목요일은 외근이구 금요일은 출장 [sep]금요일이 당진이양?[sep]아닝아닝 10일...   \n",
              "\n",
              "                                             Summary  \n",
              "0     팀장님이 출장 가서 머물 숙소를 계속해서 더 싼 데로 하게 한다고 이야기하고 있다.  \n",
              "1         이제 이력서를 쓰고 영어도 해야 한다고 해서 첫 회사를 잘 들어가라고 했다.  \n",
              "2  느낌상 대통령까지는 아니고 오시면 여사님 정도 오시지 않을까라며 이에 대해 이야기하...  \n",
              "3                      한 달 안에 무조건 일을 시작해서 돈을 벌어야 한다.  \n",
              "4                   목요일에 외근이고 금요일에 출장인데 당진은 10일에 간다.  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f64df2d3-081c-4e68-87d5-1a5c5542761e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>웅[sep]영업팀과장님이 보내줬는데 팀장님이 해줄지 모르겠다 저번에 부산갈때도 숙소...</td>\n",
              "      <td>팀장님이 출장 가서 머물 숙소를 계속해서 더 싼 데로 하게 한다고 이야기하고 있다.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>너는 잘가라....회사.... 선택 잘해..[sep]알겠어 많이 힘들구나... 나도...</td>\n",
              "      <td>이제 이력서를 쓰고 영어도 해야 한다고 해서 첫 회사를 잘 들어가라고 했다.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>느낌상 대통령까지는 아니고 오시면 여사님정도오시지않을까 [sep]그러면서[sep]샘...</td>\n",
              "      <td>느낌상 대통령까지는 아니고 오시면 여사님 정도 오시지 않을까라며 이에 대해 이야기하...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>숨만수이 도 숨만쉬어도 100 이내[sep]한달안에 일 무조건 해야대[sep]아 딱...</td>\n",
              "      <td>한 달 안에 무조건 일을 시작해서 돈을 벌어야 한다.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>목요일은 외근이구 금요일은 출장 [sep]금요일이 당진이양?[sep]아닝아닝 10일...</td>\n",
              "      <td>목요일에 외근이고 금요일에 출장인데 당진은 10일에 간다.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f64df2d3-081c-4e68-87d5-1a5c5542761e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f64df2d3-081c-4e68-87d5-1a5c5542761e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f64df2d3-081c-4e68-87d5-1a5c5542761e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DF > data Set으로 전환\n",
        "train_data = Dataset.from_pandas(train_df) \n",
        "val_data = Dataset.from_pandas(val_df)\n",
        "test_samples = Dataset.from_pandas(val_df)\n",
        "\n",
        "print(train_data)\n",
        "print(val_data)\n",
        "print(test_samples)"
      ],
      "metadata": {
        "id": "LYwdAKbOf1QJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c7b0e97-b821-4616-be92-94f9c3e43e01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['Text', 'Summary'],\n",
            "    num_rows: 279992\n",
            "})\n",
            "Dataset({\n",
            "    features: ['Text', 'Summary'],\n",
            "    num_rows: 35004\n",
            "})\n",
            "Dataset({\n",
            "    features: ['Text', 'Summary'],\n",
            "    num_rows: 35004\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52be9bda-b18c-4db5-c426-2fa2288fb0ba",
        "id": "9zJbM1A-h8vb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(30032, 768, padding_idx=3)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "model_checkpoints = \"/content/drive/MyDrive/인공지능/생성요약프로젝트/Model/KoBART/checkpoint/domain_adaptation/checkpoint-12500\"\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoints)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoints)\n",
        "\n",
        "# special_words = [\n",
        "#                 \"#@주소#\", \"#@이모티콘#\", \"#@이름#\", \"#@URL#\", \"#@소속#\",\n",
        "#                 \"#@기타#\", \"#@전번#\", \"#@계정#\", \"#@url#\", \"#@번호#\", \"#@금융#\", \"#@신원#\",\n",
        "#                 \"#@장소#\", \"#@시스템#사진#\", \"#@시스템#동영상#\", \"#@시스템#기타#\", \"#@시스템#검색#\",\n",
        "#                 \"#@시스템#지도#\", \"#@시스템#삭제#\", \"#@시스템#파일#\", \"#@시스템#송금#\", \"#@시스템#\",\n",
        "#                 \"#개인 및 관계#\", \"#미용과 건강#\", \"#상거래(쇼핑)#\", \"#시사/교육#\", \"#식음료#\", \n",
        "#                 \"#여가 생활#\", \"#일과 직업#\", \"#주거와 생활#\", \"#행사#\",\"[sep]\"\n",
        "#                 ]\n",
        "\n",
        "# tokenizer.add_special_tokens({\"additional_special_tokens\": special_words})\n",
        "# model.resize_token_embeddings(len(tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# t_len = [len(tokenizer.encode(s)) for s in tqdm(train_df['Text'])]\n",
        "# s_len = [len(tokenizer.encode(s)) for s in tqdm(train_df['Summary'])]\n",
        "\n",
        "# fig, axes = plt.subplots(1, 2, figsize=(10, 3.5), sharey=True)\n",
        "# axes[0].hist(t_len, bins=50, color=\"C0\", edgecolor=\"C0\")\n",
        "# axes[0].set_title(\"Dialogue Token Length\")\n",
        "# axes[0].set_xlabel(\"Length\")\n",
        "# axes[0].set_ylabel(\"Count\")\n",
        "# axes[1].hist(s_len, bins=50, color=\"C0\", edgecolor=\"C0\")\n",
        "# axes[1].set_title(\"Summary Token Length\")\n",
        "# axes[1].set_xlabel(\"Length\")\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "VvnQUTP48x0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8JOECsph8vc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "bd79f2784bb44c3bba82fa1bcffcb70f",
            "5ff8ae62fe874d65883c0207ccb32851",
            "c957795247d646a2aa40ee9306e4ab36",
            "5752be05a5764c9ba511fa2502111090",
            "a1669cb3953c4971b54d7800156ec043",
            "080b51efc57a473cbf2d5b4546545f4c",
            "db78740b291c42bbacb19ced469dd685",
            "e6fe494cb1094aeeac5be4e9231439d4",
            "9832476f54834be6865a9da02470313d",
            "0c3344b6fddc41caad9273e7820ad58c",
            "1fedc797f5dd4f6487367cc1c8e271be",
            "8d784853ec45455b874b2a19928a7ee3",
            "4caea79382264b50ac55ff526d5a3345",
            "7db2ec22312e4c5cbfc0c264fe152b32",
            "0651799450ba427da7fada023bf631fc",
            "bd1e7fe596364ff49fd38a56704284bd",
            "2821df255ec6474aa25bdc98aab0fcc7",
            "2e4486b753c44adba4ed6070f3efcdc1",
            "085f31861713415db84099e238f417ec",
            "dbedd0b7081945a19a115549345b6ee9",
            "e5ac5437dfd7406b966d2ad42590ae3f",
            "b36269d560d54353b320b92fa3eec61d"
          ]
        },
        "outputId": "92c3d1aa-aaf6-492d-f7ea-03e9fc5bf2b1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/280 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bd79f2784bb44c3bba82fa1bcffcb70f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/36 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8d784853ec45455b874b2a19928a7ee3"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "max_input = 256\n",
        "max_target = 64\n",
        "ignore_index = -100# tokenizer.pad_token_id\n",
        "\n",
        "def add_ignored_data(inputs, max_len, ignore_index):\n",
        "  if len(inputs) < max_len:\n",
        "      pad = [ignore_index] *(max_len - len(inputs)) # ignore_index즉 -100으로 패딩을 만들 것인데 max_len - lne(inpu)\n",
        "      inputs = np.concatenate([inputs, pad])\n",
        "  else:\n",
        "      inputs = inputs[:max_len]\n",
        "\n",
        "  return inputs\n",
        "\n",
        "def add_padding_data(inputs, max_len):\n",
        "    pad_index = tokenizer.pad_token_id\n",
        "    if len(inputs) < max_len:\n",
        "        pad = [pad_index] *(max_len - len(inputs))\n",
        "        inputs = np.concatenate([inputs, pad])\n",
        "    else:\n",
        "        inputs = inputs[:max_len]\n",
        "\n",
        "    return inputs \n",
        "\n",
        "def preprocess_data(data_to_process):\n",
        "    label_id= []\n",
        "    label_ids = []\n",
        "    dec_input_ids = []\n",
        "    input_ids = []\n",
        "    bos = tokenizer('<s>')['input_ids']\n",
        "    for i in range(len(data_to_process['Text'])):\n",
        "        input_ids.append(add_padding_data(tokenizer.encode(data_to_process['Text'][i], add_special_tokens=False), max_input))\n",
        "    for i in range(len(data_to_process['Summary'])):\n",
        "        label_id.append(tokenizer.encode(data_to_process['Summary'][i]))  \n",
        "        label_id[i].append(tokenizer.eos_token_id)   \n",
        "        dec_input_id = bos\n",
        "        dec_input_id += label_id[i][:-1]\n",
        "        dec_input_ids.append(add_padding_data(dec_input_id, max_target))  \n",
        "    for i in range(len(data_to_process['Summary'])):\n",
        "        label_ids.append(add_ignored_data(label_id[i], max_target, ignore_index))\n",
        "   \n",
        "    return {'input_ids': input_ids,\n",
        "            'attention_mask' : (np.array(input_ids) != tokenizer.pad_token_id).astype(int),\n",
        "            'decoder_input_ids': dec_input_ids,\n",
        "            'decoder_attention_mask': (np.array(dec_input_ids) != tokenizer.pad_token_id).astype(int),\n",
        "            'labels': label_ids}\n",
        "\n",
        "train_tokenize_data = train_data.map(preprocess_data, batched = True, remove_columns=['Text', 'Summary'])\n",
        "val_tokenize_data = val_data.map(preprocess_data, batched = True, remove_columns=['Text', 'Summary'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lzjlGjLh8vg"
      },
      "outputs": [],
      "source": [
        "rouge = Rouge()\n",
        "def compute_metrics(pred):\n",
        "    labels_ids = pred.label_ids\n",
        "    pred_ids = pred.predictions\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
        "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
        "    \n",
        "    return rouge.get_scores(pred_str, label_str, avg=True)   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DyoFVKwh8vh"
      },
      "outputs": [],
      "source": [
        "# model.config.max_length = 64 \n",
        "# model.config.early_stopping = True\n",
        "# model.config.no_repeat_ngram_size = 3\n",
        "# model.config.length_penalty = 2.0\n",
        "# model.config.num_beams = 5\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/인공지능/생성요약프로젝트/Model/KoBART/checkpoint2/KoBART_Summary_v3\",\n",
        "    num_train_epochs=5,  # demo\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    per_device_train_batch_size=128,  # demo\n",
        "    per_device_eval_batch_size=256,\n",
        "    learning_rate=3e-05,\n",
        "    weight_decay=0.1,\n",
        "    #label_smoothing_factor=0.1,\n",
        "    predict_with_generate=True, # 생성기능을 사용하고 싶다고 지정한다.\n",
        "    logging_dir=\"/content/drive/MyDrive/인공지능/생성요약프로젝트/Model/KoBART/logs2\",\n",
        "    save_total_limit=3,\n",
        "    load_best_model_at_end = True,\n",
        "    logging_strategy = 'epoch',\n",
        "    evaluation_strategy  = 'epoch',\n",
        "    save_strategy ='epoch',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "GrpLKYU6h8vk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c0aa0b78-d480-47b7-928a-c7b71d425aad"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 279992\n",
            "  Num Epochs = 5\n",
            "  Instantaneous batch size per device = 128\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 10940\n",
            "  Number of trainable parameters = 123884544\n",
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.13.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230126_121705-i8ocvhzh</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/jx7789/huggingface/runs/i8ocvhzh\" target=\"_blank\">/content/drive/MyDrive/인공지능/생성요약프로젝트/Model/KoBART/checkpoint2/KoBART_Summary_v3</a></strong> to <a href=\"https://wandb.ai/jx7789/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href=\"https://wandb.ai/jx7789/huggingface\" target=\"_blank\">https://wandb.ai/jx7789/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href=\"https://wandb.ai/jx7789/huggingface/runs/i8ocvhzh\" target=\"_blank\">https://wandb.ai/jx7789/huggingface/runs/i8ocvhzh</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4377' max='10940' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 4377/10940 1:49:18 < 2:43:58, 0.67 it/s, Epoch 2/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rouge-1</th>\n",
              "      <th>Rouge-2</th>\n",
              "      <th>Rouge-l</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.108000</td>\n",
              "      <td>1.834021</td>\n",
              "      <td>{'r': 0.2706503207695302, 'p': 0.2564607618284898, 'f': 0.25422936677433566}</td>\n",
              "      <td>{'r': 0.1093380499625073, 'p': 0.10461094205780852, 'f': 0.10258565319553099}</td>\n",
              "      <td>{'r': 0.2556777442143622, 'p': 0.24232975206580643, 'f': 0.24017353771835745}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='31' max='137' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 31/137 07:23 < 26:07, 0.07 it/s]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 35004\n",
            "  Batch size = 256\n",
            "Trainer is attempting to log a value of \"{'r': 0.2706503207695302, 'p': 0.2564607618284898, 'f': 0.25422936677433566}\" of type <class 'dict'> for key \"eval/rouge-1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Trainer is attempting to log a value of \"{'r': 0.1093380499625073, 'p': 0.10461094205780852, 'f': 0.10258565319553099}\" of type <class 'dict'> for key \"eval/rouge-2\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Trainer is attempting to log a value of \"{'r': 0.2556777442143622, 'p': 0.24232975206580643, 'f': 0.24017353771835745}\" of type <class 'dict'> for key \"eval/rouge-l\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Saving model checkpoint to /content/drive/MyDrive/인공지능/생성요약프로젝트/Model/KoBART/checkpoint2/KoBART_Summary_v3/checkpoint-2188\n",
            "Configuration saved in /content/drive/MyDrive/인공지능/생성요약프로젝트/Model/KoBART/checkpoint2/KoBART_Summary_v3/checkpoint-2188/config.json\n",
            "Model weights saved in /content/drive/MyDrive/인공지능/생성요약프로젝트/Model/KoBART/checkpoint2/KoBART_Summary_v3/checkpoint-2188/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/인공지능/생성요약프로젝트/Model/KoBART/checkpoint2/KoBART_Summary_v3/checkpoint-2188/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/인공지능/생성요약프로젝트/Model/KoBART/checkpoint2/KoBART_Summary_v3/checkpoint-2188/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 35004\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10940' max='10940' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10940/10940 5:54:55, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rouge-1</th>\n",
              "      <th>Rouge-2</th>\n",
              "      <th>Rouge-l</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.108000</td>\n",
              "      <td>1.834021</td>\n",
              "      <td>{'r': 0.2706503207695302, 'p': 0.2564607618284898, 'f': 0.25422936677433566}</td>\n",
              "      <td>{'r': 0.1093380499625073, 'p': 0.10461094205780852, 'f': 0.10258565319553099}</td>\n",
              "      <td>{'r': 0.2556777442143622, 'p': 0.24232975206580643, 'f': 0.24017353771835745}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.761400</td>\n",
              "      <td>1.762815</td>\n",
              "      <td>{'r': 0.2761239441756887, 'p': 0.2667388093390134, 'f': 0.26218661317352965}</td>\n",
              "      <td>{'r': 0.11504483858665394, 'p': 0.11294365892157265, 'f': 0.10949501459613435}</td>\n",
              "      <td>{'r': 0.26171317203763883, 'p': 0.25274558055702234, 'f': 0.24846189448499115}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.647200</td>\n",
              "      <td>1.746965</td>\n",
              "      <td>{'r': 0.28340056422376286, 'p': 0.2663522506232208, 'f': 0.26544272885290393}</td>\n",
              "      <td>{'r': 0.11810680880634239, 'p': 0.11185474295923847, 'f': 0.1104452157086774}</td>\n",
              "      <td>{'r': 0.2678445057637295, 'p': 0.2516211244137005, 'f': 0.25078959489815383}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.574100</td>\n",
              "      <td>1.738363</td>\n",
              "      <td>{'r': 0.282964821828852, 'p': 0.2700889496824275, 'f': 0.26707330150454234}</td>\n",
              "      <td>{'r': 0.11830275153338445, 'p': 0.11419206803790562, 'f': 0.11167558142599346}</td>\n",
              "      <td>{'r': 0.26780880510142335, 'p': 0.25550766540175024, 'f': 0.25269894695344086}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.528300</td>\n",
              "      <td>1.737903</td>\n",
              "      <td>{'r': 0.28300390498422606, 'p': 0.26869306519676533, 'f': 0.2663555577139043}</td>\n",
              "      <td>{'r': 0.11858829351475554, 'p': 0.11378410209212253, 'f': 0.11155711527854474}</td>\n",
              "      <td>{'r': 0.2677241754870815, 'p': 0.254110027564464, 'f': 0.2519211651362874}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Trainer is attempting to log a value of \"{'r': 0.2761239441756887, 'p': 0.2667388093390134, 'f': 0.26218661317352965}\" of type <class 'dict'> for key \"eval/rouge-1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Trainer is attempting to log a value of \"{'r': 0.11504483858665394, 'p': 0.11294365892157265, 'f': 0.10949501459613435}\" of type <class 'dict'> for key \"eval/rouge-2\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Trainer is attempting to log a value of \"{'r': 0.26171317203763883, 'p': 0.25274558055702234, 'f': 0.24846189448499115}\" of type <class 'dict'> for key \"eval/rouge-l\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Saving model checkpoint to /content/drive/MyDrive/인공지능/생성요약프로젝트/Model/KoBART/checkpoint2/KoBART_Summary_v3/checkpoint-4376\n",
            "Configuration saved in /content/drive/MyDrive/인공지능/생성요약프로젝트/Model/KoBART/checkpoint2/KoBART_Summary_v3/checkpoint-4376/config.json\n",
            "Model weights saved in /content/drive/MyDrive/인공지능/생성요약프로젝트/Model/KoBART/checkpoint2/KoBART_Summary_v3/checkpoint-4376/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/인공지능/생성요약프로젝트/Model/KoBART/checkpoint2/KoBART_Summary_v3/checkpoint-4376/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/인공지능/생성요약프로젝트/Model/KoBART/checkpoint2/KoBART_Summary_v3/checkpoint-4376/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 35004\n",
            "  Batch size = 256\n",
            "Trainer is attempting to log a value of \"{'r': 0.28340056422376286, 'p': 0.2663522506232208, 'f': 0.26544272885290393}\" of type <class 'dict'> for key \"eval/rouge-1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Trainer is attempting to log a value of \"{'r': 0.11810680880634239, 'p': 0.11185474295923847, 'f': 0.1104452157086774}\" of type <class 'dict'> for key \"eval/rouge-2\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Trainer is attempting to log a value of \"{'r': 0.2678445057637295, 'p': 0.2516211244137005, 'f': 0.25078959489815383}\" of type <class 'dict'> for key \"eval/rouge-l\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Saving model checkpoint to /content/drive/MyDrive/인공지능/생성요약프로젝트/Model/KoBART/checkpoint2/KoBART_Summary_v3/checkpoint-6564\n",
            "Configuration saved in /content/drive/MyDrive/인공지능/생성요약프로젝트/Model/KoBART/checkpoint2/KoBART_Summary_v3/checkpoint-6564/config.json\n",
            "Model weights saved in /content/drive/MyDrive/인공지능/생성요약프로젝트/Model/KoBART/checkpoint2/KoBART_Summary_v3/checkpoint-6564/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/인공지능/생성요약프로젝트/Model/KoBART/checkpoint2/KoBART_Summary_v3/checkpoint-6564/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/인공지능/생성요약프로젝트/Model/KoBART/checkpoint2/KoBART_Summary_v3/checkpoint-6564/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 35004\n",
            "  Batch size = 256\n",
            "Trainer is attempting to log a value of \"{'r': 0.282964821828852, 'p': 0.2700889496824275, 'f': 0.26707330150454234}\" of type <class 'dict'> for key \"eval/rouge-1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Trainer is attempting to log a value of \"{'r': 0.11830275153338445, 'p': 0.11419206803790562, 'f': 0.11167558142599346}\" of type <class 'dict'> for key \"eval/rouge-2\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Trainer is attempting to log a value of \"{'r': 0.26780880510142335, 'p': 0.25550766540175024, 'f': 0.25269894695344086}\" of type <class 'dict'> for key \"eval/rouge-l\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Saving model checkpoint to /content/drive/MyDrive/인공지능/생성요약프로젝트/Model/KoBART/checkpoint2/KoBART_Summary_v3/checkpoint-8752\n",
            "Configuration saved in /content/drive/MyDrive/인공지능/생성요약프로젝트/Model/KoBART/checkpoint2/KoBART_Summary_v3/checkpoint-8752/config.json\n",
            "Model weights saved in /content/drive/MyDrive/인공지능/생성요약프로젝트/Model/KoBART/checkpoint2/KoBART_Summary_v3/checkpoint-8752/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/인공지능/생성요약프로젝트/Model/KoBART/checkpoint2/KoBART_Summary_v3/checkpoint-8752/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/인공지능/생성요약프로젝트/Model/KoBART/checkpoint2/KoBART_Summary_v3/checkpoint-8752/special_tokens_map.json\n",
            "Deleting older checkpoint [/content/drive/MyDrive/인공지능/생성요약프로젝트/Model/KoBART/checkpoint2/KoBART_Summary_v3/checkpoint-2188] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 35004\n",
            "  Batch size = 256\n",
            "Trainer is attempting to log a value of \"{'r': 0.28300390498422606, 'p': 0.26869306519676533, 'f': 0.2663555577139043}\" of type <class 'dict'> for key \"eval/rouge-1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Trainer is attempting to log a value of \"{'r': 0.11858829351475554, 'p': 0.11378410209212253, 'f': 0.11155711527854474}\" of type <class 'dict'> for key \"eval/rouge-2\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Trainer is attempting to log a value of \"{'r': 0.2677241754870815, 'p': 0.254110027564464, 'f': 0.2519211651362874}\" of type <class 'dict'> for key \"eval/rouge-l\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Saving model checkpoint to /content/drive/MyDrive/인공지능/생성요약프로젝트/Model/KoBART/checkpoint2/KoBART_Summary_v3/checkpoint-10940\n",
            "Configuration saved in /content/drive/MyDrive/인공지능/생성요약프로젝트/Model/KoBART/checkpoint2/KoBART_Summary_v3/checkpoint-10940/config.json\n",
            "Model weights saved in /content/drive/MyDrive/인공지능/생성요약프로젝트/Model/KoBART/checkpoint2/KoBART_Summary_v3/checkpoint-10940/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/인공지능/생성요약프로젝트/Model/KoBART/checkpoint2/KoBART_Summary_v3/checkpoint-10940/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/인공지능/생성요약프로젝트/Model/KoBART/checkpoint2/KoBART_Summary_v3/checkpoint-10940/special_tokens_map.json\n",
            "Deleting older checkpoint [/content/drive/MyDrive/인공지능/생성요약프로젝트/Model/KoBART/checkpoint2/KoBART_Summary_v3/checkpoint-4376] due to args.save_total_limit\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from /content/drive/MyDrive/인공지능/생성요약프로젝트/Model/KoBART/checkpoint2/KoBART_Summary_v3/checkpoint-10940 (score: 1.7379029989242554).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=10940, training_loss=1.723816364295304, metrics={'train_runtime': 21329.1872, 'train_samples_per_second': 65.636, 'train_steps_per_second': 0.513, 'total_flos': 2.134016630194176e+17, 'train_loss': 1.723816364295304, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model) # 데이터 일괄 처리?\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model, \n",
        "    training_args,\n",
        "    train_dataset=train_tokenize_data,\n",
        "    eval_dataset=val_tokenize_data,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    #callbacks = [EarlyStoppingCallback(early_stopping_patience=2)]\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxJnl1uch8vm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6a15cdf-30d4-4c92-8f34-50745849073f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/35004 [00:00<?, ?it/s]/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1387: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 64 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "100%|██████████| 35004/35004 [2:05:16<00:00,  4.66it/s]\n"
          ]
        }
      ],
      "source": [
        "def generate_summary(test_samples, model):\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        test_samples[\"Text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_target,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    input_ids = inputs.input_ids.to(model.device)\n",
        "\n",
        "    attention_mask = inputs.attention_mask.to(model.device)\n",
        "    outputs = model.generate(input_ids, num_beams=5, no_repeat_ngram_size=3,\n",
        "                            attention_mask=attention_mask, \n",
        "                            pad_token_id=tokenizer.pad_token_id,\n",
        "                            bos_token_id=tokenizer.bos_token_id,\n",
        "                            eos_token_id=tokenizer.eos_token_id,)\n",
        "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "    return outputs, output_str\n",
        "\n",
        "summaries_after_tuning=[]\n",
        "for test_sample in tqdm(test_samples):\n",
        "    summaries_after_tuning.append(generate_summary(test_sample, model)[1])\n",
        "summaries_after_tuning = list(itertools.chain(*summaries_after_tuning))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywW4ONvxh8vn"
      },
      "outputs": [],
      "source": [
        "rouge.get_scores(summaries_after_tuning, test_samples[\"Summary\"], avg=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxvHkts-h8vo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "244d6dcf-50a0-42a8-fd01-2f31e1d6dd0e"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "idx_0 \n",
            "Summary after \n",
            "영업팀 과장님이 보내줬는데 팀장님이 해줄지 모르겠고 저번에 부산 갈 때도 숙소로 엄청 싸워서 4개월 가는 것도 아니고 3일 가는데 좀 해준다고 한다.\n",
            "\n",
            "Target summary \n",
            "팀장님이 출장 가서 머물 숙소를 계속해서 더 싼 데로 하게 한다고 이야기하고 있다.\n",
            "\n",
            "Text웅[sep]영업팀과장님이 보내줬는데 팀장님이 해줄지 모르겠다 저번에 부산갈때도 숙소로 엄청 싸워서[sep]웅 흥 4개월가는거도아니고 3일가는데 좀해주지 거 얼마한다구[sep]내말이... 아니 해봤자 3일 다 합해도 몇만원 차이인데 너무해 그때는 2일이었는데도 만원 더 싼데에서 자꾸 자라고... 넘 시러..[sep]일단\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "idx_1000 \n",
            "Summary after \n",
            "내일 아르바이트(알바) 면접을 네시 반에 가야 하는데 집에서 가면 너무 멀어서 혼자 밥 먹고 아르바이트 면접을 보고 오겠다고 한다.\n",
            "\n",
            "Target summary \n",
            "내일 채점 보조 아르바이트(알바) 면접이 4시 30분에 있다.\n",
            "\n",
            "Text내일 못 먹겠다 나 알바 면접 네시반이얌 그래서 거기 가야되는데 집에서 가면 넘 멀엉 혼자 밥먹고 알바 면접보고 오겠움여[sep]어디 면접?[sep]알바면접 뭐 채점보조[sep]알써 잘갔다와\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "idx_2000 \n",
            "Summary after \n",
            "인턴인데 아직 발표를 안 했고 포스팅은 계속 올라와서 담당자가 다를 것 같다.\n",
            "\n",
            "Target summary \n",
            "인턴 발표가 나질 않는데 포스팅은 올라와 의문을 갖는다.\n",
            "\n",
            "Text#@시스템#사진# 근데아직도 발표안함[sep]다들똑같은입장[sep]강 안했나봄 발표를[sep]왜 안하지..? 인턴인데..?[sep]그니까 근데 #@기타# 포스팅은 계속 올라놈[sep]그뭐지 담당자가다를듯 포스팅올리눈사람따로[sep]에휴 언제까지 기다려 ;\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "idx_3000 \n",
            "Summary after \n",
            "다이아 바로 옆 양쪽 부분이 평평한 건 없냐고 묻자 일자를 좋아하면 리본처럼 접히는 거 말고 일자로 해준다고 한다.\n",
            "\n",
            "Target summary \n",
            "다이아 옆을 리본처럼 접히는 것 말고 일자로 해 달라고 했다.\n",
            "\n",
            "Text다이아바로옆 양쪽부분 평평한건없어여?[sep]일자를 좋아하면[sep]저렇게 리본처럼 접히는거말구[sep]일자로 해준데 그얘기야[sep]일자가조아여[sep]알았어[sep]리본처럼접히는게머지[sep]넌 몰라\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "idx_4000 \n",
            "Summary after \n",
            "빕스는 고등학생(고딩) 이후로 안 간 것 같고 애슐리카드를 봐주던 애슐리덕구가 오랜만에 가서 설렌다.\n",
            "\n",
            "Target summary \n",
            "고등학교 이후로 빕스를 간 적이 없어서 오랜만에 갈 생각하니 설렌다고 얘기했다.\n",
            "\n",
            "Text빖스를몇년만에가서 설렌다[sep]심각한애슐리덕구여꾸만 난니가부페덕구인줄알앗는데 취향이확고했었어[sep]빕스는 고딩때이후로안간거같아[sep]애슐리카드를보여주던#@이름#이잊지모태[sep]내카드에 천얼마밖에없어써 그거밖에안모여써..하[sep]실망이다\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "idx_5000 \n",
            "Summary after \n",
            "부모님이 심은 배추가 농약을 안 쳐서 벌레가 엄청 먹어서 막걸리를 뿌렸다.\n",
            "\n",
            "Target summary \n",
            "엄마와 아빠가 심은 배추에 대해 이야기한다.\n",
            "\n",
            "Text#@시스템#사진# 엄빠가 심은 배추[sep]대박 밭두 잇냐[sep]교회앞에 조그맣게 해놧는데 농약을 안쳐서 벌레가 엄청먹어서 막걸리뿌렷어 민간요법[sep]저걸로 김장하면 되겠다[sep]야 요즘 배추값 금값이여 두포기에 3마넌인가\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "idx_6000 \n",
            "Summary after \n",
            "지금 주식에 돈을 다 물려서 생활비가 제로에 수렴했고 아르바이트(알바)도 거의 다 신카 할부로 나간다.\n",
            "\n",
            "Target summary \n",
            "주식에 돈이 다 물려서 생활비가 없다고 하니 비상금을 쓰라고 하니 그건 부모님 생신이나 어버이날에 써야 한다고 안된다고 한다.\n",
            "\n",
            "Text지금 주식에 돈 다 물려서 생활비 제로에 수렴함 흙흙 알바비도 거의다[sep]거지돼써?[sep]신카 할부로 나가는제 이게 뭐람 [sep]울디마 바버야 집에 비상금 잇자나[sep]앙대 그건[sep]앵대 [sep]엄빠 생일 or 어버이날에 써야한다고\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "idx_7000 \n",
            "Summary after \n",
            "토요일 12시에 끝나는데 홍대에 오면 46분이 걸린다고 해서 지금 일어났다.\n",
            "\n",
            "Target summary \n",
            "토요일에 12시에 끝나는 데 이동에 걸리는 시간에 대해서 이야기한다.\n",
            "\n",
            "Text마쟈 나 토요일에 아마 12시에 끝나는데 언제 만날려..?[sep]너 홍대오면 몇신데..??[sep]46분 걸린대[sep]고람 1시..? 쳐자다가 지금 일어났다..하[sep]#@이름# 서울집에서 오는겨? 아님 천안에서 오는겨,,,[sep]서울집이즤[sep]홍대 가깝지? 내가 퇴근할때 알려줄게[sep]그치 뭐 2-30분이면..\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "idx_8000 \n",
            "Summary after \n",
            "공항까지 버스를 검색하니까 한 시간 반 정도 걸려서 5시 반에는 공항 가는 버스를 타야겠다.\n",
            "\n",
            "Target summary \n",
            "공항버스를 타고 공항까지 걸리는 시간에 대해 이야기한다.\n",
            "\n",
            "Text공항가는버스 5시반에는 타야겠다 나두[sep]더 일찍 가야되는거 아니야? 공항까지 얼마나걸려?[sep]버스검색하니까 한시간반정도걸리네 흠 더일찍타야하나[sep]한시간반씩이나걸려? #@이름#이 비행기가 8시5분이니까 두시간 전부터 탑승수속할걸 난 7시50분뱅기야.. 껄껄[sep]후 몇시에타야하지[sep]4시 30분엔 타야할듯 난\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "idx_9000 \n",
            "Summary after \n",
            "여의도 에프시(FC)몰에 가는 중인데 필터를 써서 바꾸라고 한다.\n",
            "\n",
            "Target summary \n",
            "둘이 여의도 아이에프시(IFC)몰에서 만나 맛있는 것을 먹기로 했다.\n",
            "\n",
            "Text맛난거 먹자 [sep]응. 연락할겡 [sep]오키 [sep]나 가는 중. 어딨어??[sep]여의도 ifc몰[sep]오키 #@시스템#사진#[sep]오 [sep]필터 써서 바꾸시오. [sep]응\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "idx_10000 \n",
            "Summary after \n",
            "금요일 오전에 안 바쁘시면 데이트 어떠냐고 하자 목요일 저녁에 동탄에서 보자고 한다.\n",
            "\n",
            "Target summary \n",
            "목요일 저녁에 동탄에서 보던가 금요일 오전에 데이트를 하자고 한다.\n",
            "\n",
            "Text혼자 노는거 재미써여?[sep]재미업써영..... 놀자아 대전으로오시길...[sep]갈까 내일 ... 근데 가면 올때 넘 우울인디 ..[sep]금욜 오전에 안바쁘시면 데이트 어떠십니깡 힣 목욜 저녁때 동탄에서 봐도대궁[sep]다 좋습니당\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "idx_11000 \n",
            "Summary after \n",
            "저질의 상품을 팔아서 대형마트에서 대부분 저 브랜드를 판다고 하자 인터넷에서 사라고 한다.\n",
            "\n",
            "Target summary \n",
            "대형마트에서 대부분 저 브랜드의 저질의 상품을 팔아서 갈 필요가 없고 가지 말아야겠고 인터넷에서 산다.\n",
            "\n",
            "Text아...? 저질의 상품을 팔다니 저긴 갈필요가없군[sep][sep]가지말자 [sep]대형마트에서 대부분 저 브랜드 팔아[sep]에휴...[sep]그래서 난 인터넷에서 사\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "idx_12000 \n",
            "Summary after \n",
            "슈펜 신발에 검정 스키니에 위에 뭘 사야 잘 어울릴지 묻고 흰 신발은 웬만하면 다 어울린다고 한다.\n",
            "\n",
            "Target summary \n",
            "슈펜 신발과 검정 스키니에 어떤 옷이 어울릴지 이야기한다.\n",
            "\n",
            "Text내가산슈펜신발에 검정스키니에 위에를뭘사야잘어울릴까 그신발에블라우스가어울릴까[sep]괜찮을가같은데 흰신발은 웬만하면 다 어울리잖 [sep]그렇치?[sep]잉\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "idx_13000 \n",
            "Summary after \n",
            "경품 이벤트 응모했는데 1등은 노트북이고 2등은 노스페이스 패딩이다.\n",
            "\n",
            "Target summary \n",
            "경품 이벤트로 1등은 노트북, 2등은 노스페이스 패딩이 준비되어 있다.\n",
            "\n",
            "Text경품 이벤트 응모도 했오??[sep]엥 경품 이벤트는 뭐야 당첨되면 뭐주는데 ? ? [sep]그거 1등 경품이 노트북이고 2등이 노스페이스 패딩인가.. 소소한 기대 품고있다 [sep]헐 노트북...그림의떡이네 크게주네 팔백집 더맘에들어\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "idx_14000 \n",
            "Summary after \n",
            "조식 가격이 너무 비싸서 내일 전화해서 취소해 준다고 한다.\n",
            "\n",
            "Target summary \n",
            "조식을 취소하려고 했는데, 취소가 안 된다고 했다고 해서 일단 전화를 해보기로 했다.\n",
            "\n",
            "Text잠등었능가[sep]아직:-)[sep]큰일이야 #@이름# 조식 취소를 건의함[sep]핫 [sep]넘 비싸성 그래서 내일 #@이름#가 전화해서 취소해준댕 아침 잘 안먹어서 아깝댕:)[sep]취소 안된다 그랬던거 같은디 #@이모티콘#흑흑#[sep]그래?? 그래도 일단 전화해보자\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "idx_15000 \n",
            "Summary after \n",
            "우리 너무 사는 것 같다며 새로운 것도 사야겠다고 하자 오늘 저녁에 잘 들어보자고 한다.\n",
            "\n",
            "Target summary \n",
            "새로운 것도 사야 하니 그만 사기로 하고 오늘 저녁에 유튜브를 잘 들어보기로 했다.\n",
            "\n",
            "Text근니까 우리 너무 사는것 같아 새로운것도 사야지[sep]마자 그만 사야지 오늘 저녁에 잘 들어보자 우튜브 유트브[sep]그래[sep]옹\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "idx_16000 \n",
            "Summary after \n",
            "나뭇잎 펠트지로 만든 톱을 만들었는데 반만 자르면 케이크 같고 좋은 것 같다.\n",
            "\n",
            "Target summary \n",
            "상자를 원형으로 잘라서 붙이고 칠하면 괜찮을 거 같고 나뭇잎 펠트지로 만든 거를 바닥에 붙이려고 한다.\n",
            "\n",
            "Text나톱들었다지금 #@시스템#사진#[sep]톱[sep]이거 반만잘라도 케이크같고 좋은거같아여 여기다가 상자를 원형으로 달라서 붙이고 칠하면 케이크될거같아유 내생각엔 이렇게만들고싶은데 #@시스템#사진# 나뭇잎 펠트지로 만든거 바닥에붙이고[sep]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "idx_17000 \n",
            "Summary after \n",
            "선물해 준 것이 우울하고 문체가 특이해서 작가가 개성 대박인 것 같다.\n",
            "\n",
            "Target summary \n",
            "선물해 준 거 너무 우울한데 문체가 정말 특이하다며 다음에 한번 읽어보라고 말한다.\n",
            "\n",
            "Text니가선물해주는거 해준거 개 우울함 근데 문체겁나특이해[sep]나도읽어봉래[sep]담에함읽어보셈[sep][sep]작가가 진자 개성대박인듯 먼가 난해한데 이미지가 떠오름\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "idx_18000 \n",
            "Summary after \n",
            "오늘부터 9시부터 영에 다닐 예정인데 할머니(할매)들이 텃세를 부린다.\n",
            "\n",
            "Target summary \n",
            "오늘부터 9시에 수영을 다니는데 할머니들이 텃새를 부린다.\n",
            "\n",
            "Text벌써 월용일이다[sep]하 내오늘부터 아홉시 스영 다닐라거[sep]왴 그렇게일찍 일어나나[sep]#@기타# 할매들이 텃세 부린다[sep]아니 이제니더 나름 오래하지않앗나\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "idx_19000 \n",
            "Summary after \n",
            "사랑니를 뺀 데 안 아픈데 무사히 넘어갈 수 있을 거냐며 마취 풀리면 밥 먹으라고 했는데 이미 먹어버렸다고 한다.\n",
            "\n",
            "Target summary \n",
            "아직 사랑니를 뺀 데가 아프지 않고 마취 풀리면 밥을 먹으라고 하였는데 이미 먹어버렸다.\n",
            "\n",
            "Text오 아직 사랑니 뺀 데 안아픈데 무사히 넘어갈수잇를거신가[sep]나중에 욱신할걸 볼 탱탱 붓고[sep]오 슬슬.. 마취풀리면 밥먹으라는데 이미먹어버림[sep] 배가 고픈건 참지모해 ...\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "idx_20000 \n",
            "Summary after \n",
            "눈썹 문신한 게 일 년 만에 지워졌는데 엄마가 살이 얇아서 그런지 리터치 때문에 오래간다.\n",
            "\n",
            "Target summary \n",
            "엄마는 리터치를 많이 해서 눈썹 문신이 오래 지속된다는 것을 알게 되었다.\n",
            "\n",
            "Text아저씨는 눈썹문신한거 금방 사라지지않았어?[sep]일년만에 지워짐 큰이모도 그렇구[sep]엄마가 살이얇아서 오래가나?[sep]내가 리터치를 많이 했잔아[sep]그렇구나 리터치때문에 오래가는거구만\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "idx_21000 \n",
            "Summary after \n",
            "백두산 화산이 폭발할까 봐 걱정을 하고 백두산화산이 우리 사는 동안 일어난다면 이렇게 아둥바둥 살 필요가 없을 것 같다.\n",
            "\n",
            "Target summary \n",
            "실시간 검색어(실검)에 백두산 화산이 있는데 우리가 가는 동안에는 안 일어났으면 좋겠다고 했다.\n",
            "\n",
            "Text백두산화산폭발할까[sep]징조잇대?[sep]응 그런가바 실검이야 백두산화산 화산우리사는동안 일어난다면 이렇게아둥바둥살필여없을탠대 라는생각이드네[sep]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "idx_22000 \n",
            "Summary after \n",
            "신 토익이 지금 토익 시험 치는 거랑 같은 거냐고 물으니 2016년에 신토익 나온 거 온 강 그거 있길래 그거 있다고 하고 책들은 2019년 개정판이 나오길래 학교 강의로 해결해 보려고 돈도 없다고 한다.\n",
            "\n",
            "Target summary \n",
            "돈이 없어서 그냥 학교 도서관 연계해서 듣는 토익 인터넷 강의로 해결해 보려고 한다.\n",
            "\n",
            "Text언니 근데 신토익이 지금 토익 셤 치는 거랑 같은 거지? 2016년에 신토익 나온 거 온강 그거 있길래 [sep]웅 신토익이 지금토익이야 [sep]아 막 책들은 2019년 개정판 나오길래 학교 강의로 해결해 보려고 돈도 없는디....[sep]그 학교도서관 연계해서 듣는 인강 말하는거여? [sep]웅 마쟈마쟈 그거 550부터 시작해야 할 듯해\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "idx_23000 \n",
            "Summary after \n",
            "방금 볼일 끝나고 저녁 먹으러 가는 중인데 속은 괜찮은데 머리도 아프고 아직 나아지지도 않아서 조심히 먹으라고 한다.\n",
            "\n",
            "Target summary \n",
            "속이 어떠냐고 해서, 나아지지 않고 머리도 아프다고 했더니 챙겨 먹으라고 이야기한다.\n",
            "\n",
            "Text어붕 늦는거야?[sep]응 방금 볼일 끝나고 저녁먹으러 가는중[sep]속은어뗘? 아직도?[sep]응...심해지진 않는데. 나아지지도 않네... . 머리도 쫌 아프고 [sep] 챙겨먹어 조심히 더 야좋아지네[sep]응 너무 걱정마용\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "idx_24000 \n",
            "Summary after \n",
            "피시톡으로 설정 눌렀더니 숨긴 친구와 차단 친구의 숫자가 나왔다.\n",
            "\n",
            "Target summary \n",
            "카카오톡에 숨겨진 친구와 차단 친구가 꽤 많다.\n",
            "\n",
            "Text아 나아까 피시톡으로 설정 눌렀다가[sep]웅[sep]숨긴친구랑 차단친구 숫자가 나오더라고?[sep]웅 몇 명이야[sep]숨김육십몇명 차단 102[sep]와 .. 세상과 단절된 삶 아니냐 그정도면?\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "idx_25000 \n",
            "Summary after \n",
            "경희유치원 뒤에 있는 한복 숍에 픽업 간다고 하니 갔다가 전해주고 엄마 픽업을 가자고 한다.\n",
            "\n",
            "Target summary \n",
            "한복을 픽업하고 경희유치원 뒤 엄마 픽업을 가기로 한다.\n",
            "\n",
            "Text저희 샵도착했어여[sep]오키 난 한복 픽업간다[sep]일찍가는군[sep] 갔다가 전해주고 엄마픽업갈까?[sep]#@이름#가 집이랑가까운거같던데[sep] 경희유치원뒤야 한복 주고가마\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "idx_26000 \n",
            "Summary after \n",
            "교수님이 맥북에서는 영상이 안 열린다고 공지했길래 신기했다.\n",
            "\n",
            "Target summary \n",
            "가끔 학교 재학 증명서 발급해서 저장하는 프로그램같이 안 되는 것이 있지만 장점이 더 많아서 맥북을 사용한다고 한다.\n",
            "\n",
            "Text막 교수님이[sep]뭐가 안 열리지..[sep]맥북에서는 안영린다고 공지했길래 신기해써[sep]근데 나는 장점이 더 많아성 안되는거 가끔 학교 재학증명서 발급해서 저장하는 이땈 프로그램[sep]아아\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "idx_27000 \n",
            "Summary after \n",
            "현지가이드가 잘생겼는데 유부남이었고 사무장님은 결혼 여부를 물어봐서 사람을 불편하게 한다.\n",
            "\n",
            "Target summary \n",
            "현지 가이드가 잘생긴 유부남인데 사무장님이 결혼 여부를 물어서 불편했다.\n",
            "\n",
            "Text현지가이드가 잘생겼었어[sep]아 존잼이겠는데...?[sep]근데 유부남이었어[sep]아쉘 쉣... [sep]사무장님은 왜 결혼 여부를 물어봐서 사람 불편하게 하냐.. 불편한 사람은 나임 [sep]앜개웃곀\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "idx_28000 \n",
            "Summary after \n",
            "점심을 세시에 먹어서 점저를 드셨다.\n",
            "\n",
            "Target summary \n",
            "늦게 일어나서 점심을 늦게 먹었다고 하며 점심 겸 저녁(점저)이 아니라 늦은 점심이라고 말한다.\n",
            "\n",
            "Text왜캐 늦게멋엇냐 점심[sep]늦게일어났우닠가 [sep]점저를 드셧네[sep]모래 그게어케점저녀 점심이지 늦은잠심[sep]세시에 먹음 점저아닙니까\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "idx_29000 \n",
            "Summary after \n",
            "수정과처럼 작은 것을 온라인에서 팔면 사고 싶고 따뜻하게 먹고 싶다.\n",
            "\n",
            "Target summary \n",
            "작은 것은 금방 먹었고 온라인에서 팔면 수정과처럼 겨울에 먹고 싶다.\n",
            "\n",
            "Text작은거 순삭이다[sep]그치... 온라인에서 팔면 사고싶다 따뜻하게 겨율에 먹고싶다 수정과처럼[sep]그치 하.. 이제 1장씀 [sep]아\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "idx_30000 \n",
            "Summary after \n",
            "3일까지 리그 오브 레전드(롤)를 사기로 해서 너랑 만나서 하면 된다고 한다.\n",
            "\n",
            "Target summary \n",
            "우리 서로 3일까지 리그 오브 레전드(롤)를 안 하기로 했는데 연습을 해야 야 새로운 챔피언(새챔)을 할 수 있으니 너랑 만나서 해야겠다.\n",
            "\n",
            "Text우리서로 3일까지 롤 사리기로함[sep]악 안되는데 #@이름# 그전에[sep]너랑 만나서 하면됨[sep]조금 연습해야 새챔하는디[sep]얘랑은 오늘 이루로 볼수가업더[sep]아고[sep]맨날 하루씩 쉬어서 피곤해해 너랑하지모 조만간 볼꾸니까\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "idx_31000 \n",
            "Summary after \n",
            "국가시험을 보는데 망하라고 말한 게 아직도 생생하게 기억난다.\n",
            "\n",
            "Target summary \n",
            "국가시험 보는 사람에게 시험 망하라고 얘기했던 사람이 면접 볼 때도 욕해준다고 한다.\n",
            "\n",
            "Text인성 구린놈아[sep]너가 나한테 인성을 논할땐 아닌듯 난 나 국가시험 보는데 니가 나한테 시험 망해라 라고 말한거 아직도 생생하게 기억남[sep][sep]첨들어봄 그런소리[sep]아 내가 그랬어?[sep][sep]와 진짜 나느 과거에도 정말 열심히 최선을 다하며 살았구나 진짜 맘에 든다 ;; 너 이제 면접 볼 시즌 되면 그때도 취업 버전으로 욕해줌[sep]가만안둔다 그땐[sep]가만안두면 어쩔껀뎁 나랑 같이 재수하자 넌 시험 재수 난 변시재수\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "idx_32000 \n",
            "Summary after \n",
            "오늘 샌드위치밖에 안 먹었는데 돈코츠 라멘이 너무 맛이 없었다.\n",
            "\n",
            "Target summary \n",
            "오늘 음식을 조금밖에 안 먹어서 배가 고프다고 서로에게 말하고 있다.\n",
            "\n",
            "Text나 배가 너무 고프다[sep]나도[sep]...[sep]샌드위치밖에 안먹엇어 오늘[sep]돈코츠라멘이 너무맛없엇어[sep]후[sep]나도 라면3입이링 요거트하나끝[sep]다 버렷어???[sep]바렷어..\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "idx_33000 \n",
            "Summary after \n",
            "인스타그램(인스타)에서 고르곤졸라를 보다 생각나면 말하라고 한다.\n",
            "\n",
            "Target summary \n",
            "서울에서 고르곤졸라를 사다 달라고 하고 있다.\n",
            "\n",
            "Text아니 뭐 좀 사갈까 서울에서 ? 먹고싶은거 있어 ??[sep]고르곤졸라 [sep]아니 그건 서울아니어도 되자나 [sep]그럼 서울에 머잇는지 몰라[sep]보다 생각나면 말해 인스타 같은데서[sep]고르곤졸라면 됨 몇일전에 먹엇는데 또 먹고싶가[sep]되게좋아하네 서울오면 내가 맛난 곳으로 모시죠 [sep]서울언제 갈줄알고\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "idx_34000 \n",
            "Summary after \n",
            "씨그램이라는 탄산수에 3가지 맛이 있는데 라임 맛을 제일 좋아한다.\n",
            "\n",
            "Target summary \n",
            "씨그램 탄산수 중에 라임 맛을 좋아하고 코카콜라 회사인데 엘지가 코카콜라를 인수했다고 한다.\n",
            "\n",
            "Text씨그램이라는 탄산수에요 3가지맛이 있는데 저는 라임맛을 제일 좋아하죠[sep]그거 코카콜라꺼 였네요[sep]네.맞아요[sep]엘지가 인수했잖아요 코카콜라[sep]그건 몰랐던 사실인데요. 정말이에요?[sep]네 꽤 오래되었는데 잘모르시더라고요 2007년도에 인수했어요\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "idx_35000 \n",
            "Summary after \n",
            "닭갈비가 맛있었고 볶음밥도 먹고 맛있었다.\n",
            "\n",
            "Target summary \n",
            "아깐 먹기 싫다고 했지만 닭갈비 맛있었으며 역시 너의 선택은 최고이고 볶음밥도 최고라고 한다.\n",
            "\n",
            "Text닭갈비 맛있었다 맛나맛나 그티웅??[sep] 아깐 먹기 싫다더니 [sep]역시 너의 선택 최고엿어 [sep]요즘 내 메뉴선택 죽이지 ?[sep]응응 볶음밥도 채고 [sep]맛있었어\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i in range(0, len(summaries_after_tuning), 1000):\n",
        "    print('idx_{} '.format(i))\n",
        "    print(\"Summary after \\n\"+ summaries_after_tuning[i])\n",
        "    print(\"\")\n",
        "    print(\"Target summary \\n\"+ test_samples[\"Summary\"][i])\n",
        "    print(\"\")\n",
        "    print('Text'+ test_samples[\"Text\"][i])\n",
        "    print('-'*100)\n",
        "    print(\"\") "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoints = \"/content/drive/MyDrive/인공지능/생성요약프로젝트/Model/KoBART/checkpoint2/KoBART_Summary_v3/checkpoint-8752\"\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoints)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoints)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c09DPdvwQNt3",
        "outputId": "8bbce7d8-6573-4407-e7bb-a55d7b26786c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub(\n",
        "    \"kobart_summary_v3\", \n",
        "    use_temp_dir=True, \n",
        "    use_auth_token=\"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\"\n",
        ")\n",
        "tokenizer.push_to_hub(\n",
        "    \"kobart_summary_v3\", \n",
        "    use_temp_dir=True, \n",
        "    use_auth_token=\"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\"\n",
        ")"
      ],
      "metadata": {
        "id": "_gzjYvmDoFSz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117,
          "referenced_widgets": [
            "48454c4378a74c42a2a241c1eb35877b",
            "29e92b581b4a40c69a2be33964b9a6f5",
            "34cb31a3eead4b409eb91b40de640734",
            "12f6e33c989d471d8e2e69953a54530d",
            "f4b53872ea6f4bbfbd5050aed1433f56",
            "d9bd459e812a4bc493aa4c733e7f05f1",
            "20273645d167416b82924a8beaecf1f3",
            "e8c35cf03d31424c9ef61a6bc4e4e182",
            "eb58eca329384b8b8769d14d2fe56211",
            "519f77c206504517af5a00a39a0b616d",
            "f618554e8d974cdd85534156d5d9856d",
            "f04575ee0dcc448fac639ba1aa337110",
            "3889e1831608437ea7b0a24d2a8de091",
            "1fcf8d5c004542a29acad3bf57e05ef4",
            "0f1eb613ccfe48b18dd9f6d09b9dfff7",
            "2f44ce41ad8b4600a82b137772c72575",
            "5a6039c07de249cdb2400820fd0247d0",
            "54cc95e987f6407188539c5b32b57f9e",
            "00db76a48fab493087645db371c6deed",
            "88abde0827d54afb8a59dae2fe14449b",
            "a6321fdec08940288aa3e7be8f1ba09e",
            "b3586d5d1f314617a976cd3d0eb361dc"
          ]
        },
        "outputId": "d9424e51-4a30-42ca-bcff-5e5e101a44e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/496M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "48454c4378a74c42a2a241c1eb35877b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f04575ee0dcc448fac639ba1aa337110"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/jx7789/kobart_summary_v3/commit/7c4705b4d914ef85bc6aeaefe3fabd8b4cf744e8', commit_message='Upload tokenizer', commit_description='', oid='7c4705b4d914ef85bc6aeaefe3fabd8b4cf744e8', pr_url=None, pr_revision=None, pr_num=None)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IVGtQOsQQr0K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
